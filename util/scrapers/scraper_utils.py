#!/usr/bin/env python3
"""
Shared utilities for all scrapers.
Common functions, constants, and helpers to reduce duplication.
"""

from pathlib import Path
import requests
import time
import os
import re
from typing import Optional
import sys

# Add parent directory to path for util imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

# Import and re-export reference resolution functions from misc_utils
from util.misc_utils import name_to_enum, build_all_item_lookups, resolve_item_reference

# ============================================================================
# PATH HELPERS
# ============================================================================

def get_cache_dir(scraper_name: str) -> Path:
    """Get cache directory for a scraper (e.g., 'equipment' -> '../cache/equipment_cache')"""
    return Path(__file__).parent.parent / 'cache' / f'{scraper_name}_cache'

def get_cache_file(filename: str) -> Path:
    """Get cache file path (e.g., 'services_cache.html' -> '../cache/services_cache.html')"""
    return Path(__file__).parent.parent / 'cache' / filename

def get_output_file(filename: str) -> Path:
    """Get output file path in autogenerated folder"""
    return Path(__file__).parent.parent / 'autogenerated' / filename


# ============================================================================
# FOLDER SCANNING
# ============================================================================

def scan_cache_folder_for_items(cache_dir: Path, main_cache_file: Path, skip_items: list = None) -> list:
    """
    Scan cache folder for individual item HTML files.
    
    This allows adding new items before they appear on the main wiki page.
    Files should be named like "Item_Name.html" in the cache folder.
    
    Args:
        cache_dir: Path to cache directory
        main_cache_file: Path to main cache file (will be excluded)
        skip_items: Optional list of item names to skip
    
    Returns:
        List of item dicts with:
        - name: Item name (extracted from filename)
        - url: None (no URL for folder-scanned items)
        - from_folder: True (marks as folder-scanned)
        - cache_file: Path to the HTML file
    """
    if not cache_dir.exists():
        print(f"  Cache folder doesn't exist: {cache_dir}")
        return []
    
    items = []
    html_files = list(cache_dir.glob('*.html'))
    
    # Filter out the main cache file
    main_cache_name = main_cache_file.name if hasattr(main_cache_file, 'name') else str(main_cache_file).split('/')[-1]
    html_files = [f for f in html_files if f.name != main_cache_name]
    
    if not html_files:
        return []
    
    print(f"  Found {len(html_files)} HTML files in cache folder")
    
    skip_items = skip_items or []
    
    for html_file in html_files:
        # Extract item name from filename (remove .html extension)
        item_name = html_file.stem
        
        # Decode URL encoding in filename (e.g., Widow%27s_kiss -> Widow's_kiss)
        from urllib.parse import unquote
        item_name = unquote(item_name)
        
        # Convert underscores back to spaces if needed
        item_name = item_name.replace('_', ' ')
        
        # Skip if in skip list
        if item_name in skip_items:
            continue
        
        print(f"  Found: {item_name}")
        
        items.append({
            'name': item_name,
            'url': None,  # No URL for folder-scanned items
            'from_folder': True,  # Mark as folder-scanned
            'cache_file': html_file  # Store the file path
        })
    
    return items


def merge_folder_items_with_main_list(main_list: list, folder_items: list, name_key: str = 'name') -> list:
    """
    Merge folder-scanned items with main list, avoiding duplicates.
    
    Args:
        main_list: List of items from main wiki page
        folder_items: List of items from folder scan
        name_key: Key to use for name comparison (default: 'name')
    
    Returns:
        Merged list with folder items added (duplicates skipped)
    """
    if not folder_items:
        return main_list
    
    print(f"\nMerging {len(folder_items)} folder items with {len(main_list)} main items...")
    
    # Build set of existing names
    existing_names = {item[name_key] for item in main_list}
    
    added_count = 0
    skipped_count = 0
    
    for folder_item in folder_items:
        if folder_item[name_key] not in existing_names:
            main_list.append(folder_item)
            print(f"  ✓ Added: {folder_item[name_key]}")
            added_count += 1
        else:
            print(f"  ⊘ Skipped duplicate: {folder_item[name_key]}")
            skipped_count += 1
    
    print(f"\nMerge complete: {added_count} added, {skipped_count} skipped")
    print(f"Total items to process: {len(main_list)}")
    
    return main_list


def read_cached_html(cache_path: Path) -> Optional[str]:
    """
    Read HTML from a cached file.
    
    Args:
        cache_path: Path to cached HTML file
    
    Returns:
        HTML content or None if failed
    """
    try:
        with open(cache_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        print(f"  ERROR reading {cache_path}: {e}")
        return None


# ============================================================================
# DOWNLOAD HELPERS
# ============================================================================

def download_page(url: str, cache_path: Path, rescrape: bool = False) -> Optional[str]:
    """
    Download a page from URL and cache it.
    
    Args:
        url: URL to download
        cache_path: Path to cache file
        rescrape: If True, re-download even if cached
        
    Returns:
        HTML content or None if failed
    """
    if cache_path.exists() and not rescrape:
        return cache_path.read_text(encoding='utf-8')
    
    try:
        print(f"  Downloading {url}...")
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        cache_path.parent.mkdir(parents=True, exist_ok=True)
        cache_path.write_text(response.text, encoding='utf-8')
        
        return response.text
    except Exception as e:
        print(f"  ERROR downloading {url}: {e}")
        return None


def download_with_retry(url: str, cache_path: Path, max_retries: int = 3, delay: int = 5) -> Optional[str]:
    """
    Download with retry logic for Lua errors.
    
    Args:
        url: URL to download
        cache_path: Path to cache file
        max_retries: Maximum number of retry attempts
        delay: Seconds to wait between retries
        
    Returns:
        HTML content or None if all retries failed
    """
    # First try to load from cache
    html = download_page(url, cache_path, rescrape=False)
    if html and 'Lua error' not in html and 'ScribuntoErrors' not in html:
        return html
    
    # If cache has Lua error or doesn't exist, retry with downloads
    for attempt in range(max_retries):
        html = download_page(url, cache_path, rescrape=True)
        if html and 'Lua error' not in html and 'ScribuntoErrors' not in html:
            return html
        
        if attempt < max_retries - 1:
            print(f"  Lua error detected, retrying in {delay}s... (attempt {attempt + 1}/{max_retries})")
            time.sleep(delay)
    
    print(f"  WARNING: Failed to download {url} after {max_retries} attempts")
    return None


# ============================================================================
# TEXT PROCESSING
# ============================================================================

def clean_text(text: str) -> str:
    """Clean up text by removing extra whitespace."""
    return ' '.join(text.split()).strip()


def clean_wiki_url(url: str) -> str:
    """
    Clean up wiki URLs by removing Special:MyLanguage/ prefixes.
    
    Args:
        url: Wiki URL that may contain Special:MyLanguage/ prefix
    
    Returns:
        Cleaned URL without the prefix
    
    Example:
        '/wiki/Special:MyLanguage/Item_Name' -> '/wiki/Item_Name'
    """
    if not url:
        return url
    
    # Remove Special:MyLanguage/ prefix
    url = url.replace('/Special:MyLanguage/', '/')
    url = url.replace('Special:MyLanguage/', '')
    
    return url


def sanitize_filename(name: str) -> str:
    """Convert item name to safe filename."""
    # Remove or replace characters that are problematic in filenames
    name = name.replace('/', '_')
    name = name.replace('\\', '_')
    name = name.replace(':', '_')
    name = name.replace('*', '_')
    name = name.replace('?', '_')
    name = name.replace('"', '_')
    name = name.replace('<', '_')
    name = name.replace('>', '_')
    name = name.replace('|', '_')
    return name


# ============================================================================
# STAT KEYWORDS
# ============================================================================

# Comprehensive stat name mappings from wiki text to internal names
# These are ALL the standard attributes that should be parsed
STAT_KEYWORDS = {
    # Core crafting/activity stats
    'work efficiency': 'work_efficiency',
    'double action': 'double_action',
    'double rewards': 'double_rewards',
    'no materials consumed': 'no_materials_consumed',
    'quality outcome': 'quality_outcome',
    
    # Steps (can be number or percent)
    'steps required': 'steps',  # Will be parsed as steps_add or steps_percent
    
    # Experience (can be number or percent)
    'bonus experience': 'bonus_xp',  # Will be parsed as bonus_xp_add or bonus_xp_percent
    'bonus xp': 'bonus_xp',
    
    # Finding stats
    'chest finding': 'chest_finding',
    'find bird nests': 'find_bird_nests',
    'find collectibles': 'find_collectibles',
    'find gems': 'find_gems',
    'fine material finding': 'fine_material_finding',
    
    # Inventory
    'inventory space': 'inventory_space',
}

# Stat types that can be either flat numbers OR percentages
# These need special handling to detect which format is used
DUAL_FORMAT_STATS = {
    'steps',  # Can be "-2 steps" or "-5% steps"
    'bonus_xp',  # Can be "+10 xp" or "+5% xp"
}

# Skills that can appear in stat descriptions
SKILL_KEYWORDS = [
    'agility', 'carpentry', 'cooking', 'crafting', 'fishing',
    'foraging', 'mining', 'smithing', 'trinketry', 'woodcutting',
    'traveling', 'global'
]

# Activities (not skills) - stats for these should be excluded or handled separately
ACTIVITY_KEYWORDS = [
    'sledding',
]


# ============================================================================
# SKILL AND LOCATION EXTRACTION HELPERS
# ============================================================================

def extract_skill_from_text(text: str) -> str:
    """
    Extract skill name from text like 'While doing Fishing' or 'Fishing'.
    Also handles 'gathering skills' and 'artisan skills'.
    
    Returns:
        Skill name in lowercase, or 'global' if no skill found
    """
    text_lower = text.lower()
    
    # Check for skill groups first
    if 'gathering skills' in text_lower or 'gathering skill' in text_lower:
        return 'gathering'
    if 'artisan skills' in text_lower or 'artisan skill' in text_lower:
        return 'artisan'
    if 'utility skills' in text_lower or 'utility skill' in text_lower:
        return 'utility'
    
    # Check for "while doing X" pattern
    while_doing_match = re.search(r'while doing\s+(\w+)', text_lower)
    if while_doing_match:
        return while_doing_match.group(1)
    
    # Check for "while X" pattern (e.g., "While traveling")
    while_match = re.search(r'while\s+(\w+)', text_lower)
    if while_match:
        skill = while_match.group(1).rstrip('.')
        if skill in SKILL_KEYWORDS:
            return skill
    
    # Check if any skill keyword appears in the text
    for skill in SKILL_KEYWORDS:
        if skill in text_lower:
            return skill
    
    return 'global'


def extract_location_from_text(text: str) -> tuple[str | None, bool]:
    """
    Extract location requirement from text like:
    - 'While in Jarvonia'
    - 'While in the underwater location'
    - '(NOT) While in Underwater location'
    - '(NOT) Not in an Underwater location'
    
    Returns:
        Tuple of (location_name, is_negated)
        - location_name: Location name in lowercase, or None if no location found
        - is_negated: True if location is negated (NOT while in X)
    """
    text_lower = text.lower()
    
    # Check for new negation format: "(NOT) Not in an X location"
    if 'not in an' in text_lower:
        is_negated = True
        location_match = re.search(r'not in an\s+([^.]+?)(?:\s+location)?\.?$', text_lower)
        if location_match:
            location = location_match.group(1).strip()
            return location, is_negated
        return None, False
    
    # Check for old format: "While in X"
    if 'while in' not in text_lower:
        return None, False
    
    # Check for negation in old format
    is_negated = '(not)' in text_lower or 'not while in' in text_lower
    
    # Extract location
    location_match = re.search(r'while in (?:the )?([^.]+?)(?:\s+(?:location|area))?\.?$', text_lower)
    if location_match:
        location = location_match.group(1).strip()
        return location, is_negated
    
    return None, False


def is_activity_stat(text: str) -> bool:
    """
    Check if text refers to an activity (not a skill).
    
    Returns:
        True if the text contains an activity reference
    """
    text_lower = text.lower()
    
    # Check for "while doing X" where X is an activity
    while_doing_match = re.search(r'while doing\s+(\w+)', text_lower)
    if while_doing_match:
        doing_what = while_doing_match.group(1)
        return doing_what in ACTIVITY_KEYWORDS
    
    return False


def normalize_location_name(location_text: str) -> str:
    """
    Normalize location name to standard format.
    
    Args:
        location_text: Raw location text from wiki
    
    Returns:
        Normalized location name ('underwater', 'jarvonia', 'gdte', 'spectral', or sanitized name)
    """
    location_lower = location_text.lower()
    return location_lower.replace(' ', '_')


# ============================================================================
# PARSING HELPERS
# ============================================================================

def parse_percentage(text: str) -> float:
    """Parse percentage from text (e.g., '5%' -> 5.0, '5.5%' -> 5.5)"""
    text = text.strip().replace('%', '').replace('+', '')
    try:
        return float(text)
    except ValueError:
        return 0.0


def parse_number(text: str) -> float:
    """Parse number from text, handling +/- signs"""
    text = text.strip().replace('+', '')
    try:
        return float(text)
    except ValueError:
        return 0.0


def parse_stat_value(text: str, stat_name: str) -> tuple[str, float]:
    """
    Parse a stat value and determine if it's a percentage or flat number.
    
    Args:
        text: The text containing the stat value (e.g., "-5%", "+10", "5.5%")
        stat_name: The base stat name (e.g., 'steps', 'bonus_xp')
    
    Returns:
        Tuple of (final_stat_name, value)
        - For percentages: ('steps_percent', 5.0) or ('bonus_xp_percent', 10.0)
        - For flat numbers: ('steps_add', -2.0) or ('bonus_xp_add', 10.0)
    """
    text = text.strip()
    
    # Check if it's a percentage
    if '%' in text:
        value = parse_percentage(text)
        if stat_name in DUAL_FORMAT_STATS:
            return (f'{stat_name}_percent', value)
        else:
            return (stat_name, value)
    else:
        # It's a flat number
        value = parse_number(text)
        if stat_name in DUAL_FORMAT_STATS:
            return (f'{stat_name}_add', value)
        else:
            return (stat_name, value)


def normalize_stat_name(text: str) -> Optional[str]:
    """
    Normalize a stat name from wiki text to internal name.
    
    Args:
        text: Wiki text like "Work Efficiency", "Double Action", etc.
    
    Returns:
        Internal stat name like 'work_efficiency', 'double_action', or None if not recognized
    """
    text_lower = text.lower().strip()
    
    # Direct lookup
    if text_lower in STAT_KEYWORDS:
        return STAT_KEYWORDS[text_lower]
    
    # Try partial matches for common variations
    for wiki_name, internal_name in STAT_KEYWORDS.items():
        if wiki_name in text_lower or text_lower in wiki_name:
            return internal_name
    
    print(f"Could not find existing stat name for {text}")
    return None


def extract_skill_from_text(text: str) -> Optional[str]:
    """Extract skill name from text like 'While doing Fishing'"""
    text_lower = text.lower()
    
    for skill in SKILL_KEYWORDS:
        if skill in text_lower:
            return skill
    
    return None


def is_activity_stat(text: str) -> bool:
    """Check if text refers to an activity (not a skill)"""
    text_lower = text.lower()
    return any(activity in text_lower for activity in ACTIVITY_KEYWORDS)


# ============================================================================
# MODULE GENERATION HELPERS
# ============================================================================

def write_module_header(f, description: str, scraper_name: str):
    """Write standard module header."""
    f.write('#!/usr/bin/env python3\n')
    f.write('"""\n')
    f.write(f'{description}\n')
    f.write(f'Auto-generated by {scraper_name}\n')
    f.write('DO NOT EDIT MANUALLY\n')
    f.write('"""\n\n')


def write_imports(f, imports: list):
    """Write import statements."""
    for imp in imports:
        f.write(f'{imp}\n')
    f.write('\n')


def write_lines(f, lines):
    """Write a list of lines to file (convenience helper)."""
    for line in lines:
        f.write(line + '\n')


# ============================================================================
# VALIDATION AND REPORTING
# ============================================================================

class ScraperValidator:
    """Tracks and reports parsing issues across scrapers."""
    
    def __init__(self):
        self.items_with_issues = []
        self.unrecognized_stats = []
    
    def add_item_issue(self, item_name: str, reasons: list):
        """Track an item with parsing issues."""
        self.items_with_issues.append({
            'name': item_name,
            'reasons': reasons
        })
    
    def add_unrecognized_stat(self, item_name: str, stat_text: str):
        """Track an unrecognized stat."""
        if item_name and item_name != 'Unknown':
            self.unrecognized_stats.append({
                'item': item_name,
                'text': stat_text
            })
    
    def validate_item_stats(self, item_name: str, stats: dict) -> list:
        """
        Validate item stats and return list of issues found.
        
        Args:
            item_name: Name of the item
            stats: Stats dict to validate
        
        Returns:
            List of issue descriptions
        """
        issues = []
        
        if not stats:
            return issues
        
        # Check for None keys
        if None in stats:
            issues.append("None skill key in stats")
        
        # Check for empty stat dicts
        for skill, locations in stats.items():
            if isinstance(locations, dict):
                for loc, stat_values in locations.items():
                    if isinstance(stat_values, dict) and len(stat_values) == 0:
                        issues.append(f"Empty stats for {skill}/{loc}")
        
        return issues
    
    def report(self):
        """Print validation report."""
        if self.items_with_issues:
            print(f"\n⚠️  Items with parsing issues ({len(self.items_with_issues)}):")
            for item in self.items_with_issues:
                print(f"  - {item['name']}: {', '.join(item['reasons'])}")
        
        if self.unrecognized_stats:
            # Group by stat text pattern
            stats_by_pattern = {}
            for stat_info in self.unrecognized_stats:
                text = stat_info['text']
                item_name = stat_info['item']
                
                # Normalize the pattern more carefully
                # Replace numbers but preserve important keywords
                pattern = re.sub(r'\b\d+(?:\.\d+)?\b', 'X', text)  # Replace standalone numbers with X
                
                # Don't normalize set names or important keywords - just use the text as-is
                # This way "Adventuring tool set" and "Proper gear" stay distinct
                
                if pattern not in stats_by_pattern:
                    stats_by_pattern[pattern] = {
                        'example': text,
                        'items': set()  # Use set to avoid duplicates
                    }
                stats_by_pattern[pattern]['items'].add(item_name)
            
            # Sort patterns by frequency (most common first)
            sorted_patterns = sorted(stats_by_pattern.items(), key=lambda x: len(x[1]['items']), reverse=True)
            
            print(f"\n⚠️  Unrecognized stats found ({len(self.unrecognized_stats)} total, {len(stats_by_pattern)} unique patterns):")
            limit = 22
            for pattern, info in sorted_patterns[:limit]:  # Show first 10 patterns
                items_list = sorted(list(info['items']))  # Convert set to sorted list
                count = len(items_list)
                items_str = ', '.join(items_list[:5])  # Show first 5 items
                if count > 5:
                    items_str += f" (+{count - 5} more)"
                print(f"  - [{count}x] '{info['example']}'")
                print(f"    Items: {items_str}")
            if len(sorted_patterns) > limit:
                print(f"  ... and {len(sorted_patterns) - limit} more patterns")
